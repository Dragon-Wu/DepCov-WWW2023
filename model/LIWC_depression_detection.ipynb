{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import liwc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modin.pandas as pd\n",
    "# import ray\n",
    "# ray.init(num_cpus=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from utils import preprocess\n",
    "from utils.tool_simple import get_keywords, list_to_txt, txt_to_list, list_drop_duplicate, many_list_count_sum, list_clean_blank, json_to_dict, dict_to_json\n",
    "from data.dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_case = 'max'\n",
    "max_length_tweet = 'max'\n",
    "path_dir_data = f\"\"\n",
    "path_dir_record = f\"\"\n",
    "if not os.path.exists(path_dir_record):\n",
    "    os.mkdir(path_dir_record)\n",
    "logger = init_logger(path_dir_record+'liwc.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_negative_all = json_to_dict(\n",
    "    os.path.join(path_dir_data, f\"dict_user_negative.json\")\n",
    ")\n",
    "dict_positive_all = json_to_dict(\n",
    "    os.path.join(path_dir_data, f\"dict_user_positive.json\")\n",
    ")\n",
    "logger.info(f\"Loading negative: {len(dict_negative_all)}\")\n",
    "logger.info(f\"Loading positive: {len(dict_positive_all)}\")\n",
    "\n",
    "num_case = (\n",
    "    int(num_case) if not isinstance(num_case, str) else len(dict_positive_all)\n",
    ")\n",
    "dict_negative = get_dict_part(\n",
    "    dict_negative_all,\n",
    "    num_case * len(dict_negative_all) / len(dict_positive_all),\n",
    "    shuffle=False,\n",
    ")\n",
    "dict_positive = get_dict_part(dict_positive_all, num_case, shuffle=False)\n",
    "\n",
    "num_negative, num_positive, list_data_negative, list_data_positive = process_data_merge(\n",
    "    dict_negative, dict_positive, max_length_tweet=max_length_tweet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = list_data_negative + list_data_positive\n",
    "list_label = [0]*num_negative + [1]*num_positive\n",
    "data_train, data_test, label_train, label_test = train_test_split(list_data, list_label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    f\"Training Size: {len(data_train)}, {sum(label_train)} positive and {len(label_train)-sum(label_train)} negative\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"Testing Size: {len(data_test)}, {sum(label_test)} positive and {len(label_test)-sum(label_test)} negative\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIWC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIWC_parse, category_names = liwc.load_token_parser('../resources/LIWC2015_English.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(LIWC_parse(\"accept\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(text):\n",
    "#     # you may want to use a smarter tokenizer\n",
    "#     for match in re.finditer(r'\\w+', text, re.UNICODE):\n",
    "#         yield match.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettysburg = '''Four score and seven years ago our fathers brought forth on\n",
    "  this continent a new nation, conceived in liberty, and dedicated to the\n",
    "  proposition that all men are created equal. Now we are engaged in a great\n",
    "  civil war, testing whether that nation, or any nation so conceived and so\n",
    "  dedicated, can long endure. We are met on a great battlefield of that war.\n",
    "  We have come to dedicate a portion of that field, as a final resting place\n",
    "  for those who here gave their lives that that nation might live. It is\n",
    "  altogether fitting and proper that we should do this.'''.lower()\n",
    "  \n",
    "gettysburg_tokens = word_tokenize(gettysburg)\n",
    "gettysburg_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettysburg_counts = Counter(category for token in gettysburg_tokens for category in LIWC_parse(token))\n",
    "# logger.info(gettysburg_counts)\n",
    "#=> Counter({'funct': 58, 'pronoun': 18, 'cogmech': 17, ...})\n",
    "dict(gettysburg_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIWC feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liwc_count(data_label):\n",
    "    data_user, label_user = data_label\n",
    "    data_user = word_tokenize(data_user)\n",
    "    liwc_count = Counter(category for token in data_user for category in LIWC_parse(token))\n",
    "    dict_liwc_counts = dict(liwc_count)\n",
    "    # length_words = sum(dict_liwc_counts.values())\n",
    "    # for category in dict_liwc_counts.keys():\n",
    "    #     dict_liwc_counts[category] = dict_liwc_counts[category] / length_words\n",
    "    dict_liwc_counts['word_length'] = sum(dict_liwc_counts.values())\n",
    "    dict_liwc_counts['label'] = label_user\n",
    "    return dict_liwc_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_label_train = list(zip(data_train, label_train))\n",
    "list_data_label_test = list(zip(data_test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 32\n",
    "pbar_data = tqdm(list_data_label_train)\n",
    "pool = multiprocessing.Pool(num_threads)\n",
    "list_dict_liwc_counts_train = pool.map(get_liwc_count, pbar_data)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 32\n",
    "pbar_data = tqdm(list_data_label_test)\n",
    "pool = multiprocessing.Pool(num_threads)\n",
    "list_dict_liwc_counts_test = pool.map(get_liwc_count, pbar_data)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_dict_liwc_counts_train), len(list_dict_liwc_counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liwc_train = pd.DataFrame(list_dict_liwc_counts_train)\n",
    "df_liwc_train = df_liwc_train.fillna(0)\n",
    "df_liwc_test = pd.DataFrame(list_dict_liwc_counts_test)\n",
    "df_liwc_test = df_liwc_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liwc_train = df_liwc_train.append(df_liwc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label == 0\n",
    "df_liwc_train_0 = df_liwc_train[df_liwc_train['label']==0]\n",
    "sum_word_length_0 = sum(df_liwc_train_0['word_length'])\n",
    "count_0 = pd.Series(df_liwc_train_0.drop(columns=['word_length', 'label'], axis=1).apply(sum), name='count_0')\n",
    "p_0 = pd.Series(df_liwc_train_0.drop(columns=['word_length', 'label'], axis=1).apply(sum)/sum_word_length_0, name='p_0')\n",
    "# label == 1\n",
    "df_liwc_train_1 = df_liwc_train[df_liwc_train['label']==1]\n",
    "sum_word_length_1 = sum(df_liwc_train_1['word_length'])\n",
    "count_1 = pd.Series(df_liwc_train_1.drop(columns=['word_length', 'label'], axis=1).apply(sum), name='count_1')\n",
    "p_1 = pd.Series(df_liwc_train_1.drop(columns=['word_length', 'label'], axis=1).apply(sum)/sum_word_length_1, name='p_1')\n",
    "\n",
    "# merge\n",
    "df_count = pd.DataFrame({count_0.name:count_0, p_0.name:p_0, count_1.name:count_1, p_1.name:p_1})\n",
    "df_count = df_count.sort_values(by='count_1', ascending=False)\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_word_length_0, sum_word_length_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_category_c_p = df_count.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significant_occurrence(name, occurrence_before, occurrence_after, count_before, count_after,  correction=False):\n",
    "    not_after = count_after-occurrence_after\n",
    "    not_before = count_before-occurrence_before\n",
    "    # build 2*2 table\n",
    "    df_chi2 = pd.DataFrame(columns=['occurrence','Not', 'Sum'], index=['after', 'before'])\n",
    "    df_chi2.loc['after'] = [occurrence_after, not_after, count_after]\n",
    "    df_chi2.loc['before'] = [occurrence_before, not_before, count_before]\n",
    "    # cal\n",
    "    chi2, P, dof, ex = chi2_contingency(df_chi2.drop('Sum',axis=1).values, correction=correction)\n",
    "    OR = (occurrence_after*not_before) / (occurrence_before*not_after)\n",
    "    Mie = 1.96/np.sqrt(chi2)\n",
    "    interval_Mie = [ np.power(OR, 1-Mie), np.power(OR, 1+Mie) ]\n",
    "    interval_Mie = np.around(interval_Mie, 2)\n",
    "    \n",
    "    return df_chi2, chi2, P, OR, interval_Mie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category_chi2 = pd.DataFrame(columns=['category','OR','P','95%CI','Chi2','Count_0','Count_1'])\n",
    "for category in dict_category_c_p.keys():\n",
    "    occurrence_0, occurrence_1 = dict_category_c_p[category]['count_0'], dict_category_c_p[category]['count_1']\n",
    "    p_0, p_1 = dict_category_c_p[category]['p_0'], dict_category_c_p[category]['p_1']\n",
    "    df_chi2, chi2, P, OR, interval_Mie = significant_occurrence(category, occurrence_before=occurrence_0, occurrence_after=occurrence_1, count_before=sum_word_length_0, count_after=sum_word_length_1)\n",
    "    s_before = f\"{int(occurrence_0)} ({p_0*100:1f}%)\"\n",
    "    s_after = f\"{int(occurrence_1)} ({p_1*100:.1f}%)\"\n",
    "    df_category_chi2.loc[len(df_category_chi2)] = [category, OR, P, interval_Mie, chi2, s_before, s_after]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category_chi2 = df_category_chi2.sort_values(by='P', ascending=True)\n",
    "df_category_chi2 = df_category_chi2[df_category_chi2['P']<0.0001]\n",
    "df_category_chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category_chi2_more = df_category_chi2[df_category_chi2['OR']>1].sort_values(by='OR', ascending=False)\n",
    "df_category_chi2_less = df_category_chi2[df_category_chi2['OR']<1].sort_values(by='OR', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category_chi2_more[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category_chi2_more.to_excel(\"p2n_df_category_chi2_more.xlsx\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category_chi2_less[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category_chi2_less.to_excel(\"p2n_df_category_chi2_less.xlsx\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.stats.proportion import proportions_ztest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_category_top = df_category_chi2.category.tolist()[:20]\n",
    "list_category_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure(data=[\n",
    "#     # go.Bar(name='General', x=list_SNOMED_top, y=df_SNOMED_body_count_percent[:22].percent),\n",
    "#     go.Bar(name='0', x=list_category_top, y=[ dict_category_c_p[category]['p_0'] for category in list_category_top ]),\n",
    "#     go.Bar(name='1', x=list_category_top, y=[ dict_category_c_p[category]['p_1'] for category in list_category_top ])\n",
    "# ])\n",
    "# # Change the bar mode\n",
    "# fig.update_layout(barmode='group')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_1 = 'indianred' \n",
    "color_2 = 'lightsalmon' \n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list_category_top,\n",
    "    y=[ dict_category_c_p[category]['p_0'] for category in list_category_top ],\n",
    "    name='0',\n",
    "    marker_color=color_1,\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list_category_top,\n",
    "    y=[ dict_category_c_p[category]['p_1'] for category in list_category_top ],\n",
    "    name='1',\n",
    "    marker_color=color_2,\n",
    "))\n",
    "\n",
    "# Here we modify the tickangle of the xaxis, resulting in rotated labels.\n",
    "fig.update_layout(\n",
    "    # title='Symptoms Prevalence of Different variants',\n",
    "    xaxis_tickfont_size=15,\n",
    "    xaxis_tickangle=-45,\n",
    "    yaxis=dict(\n",
    "        title='Prevalence(%)',\n",
    "        titlefont_size=16,\n",
    "        tickfont_size=14,\n",
    "        ticksuffix='%',\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0.95,\n",
    "        y=1.0,\n",
    "        bgcolor='rgba(255, 255, 255, 0)',\n",
    "        bordercolor='rgba(255, 255, 255, 0)',\n",
    "        font_size=15\n",
    "    ),\n",
    "    barmode='group',\n",
    "    bargap=0.1, # gap between bars of adjacent location coordinates.\n",
    "    bargroupgap=0.0, # gap between bars of the same location coordinate.\n",
    "    height=500,\n",
    "    width=1000,\n",
    "    template='simple_white'\n",
    ")\n",
    "# fig.write_image(path_dir_figure3+\"symptoms_different_variant.svg\")\n",
    "# fig.write_image(path_dir_figure3+\"symptoms_different_variant.pdf\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类报告\n",
    "from sklearn.metrics import classification_report\n",
    "# 混淆矩阵\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC曲线与AUC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "# PR曲线\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train_vec = df_liwc_train.drop(columns=['word_length','label'])\n",
    "cols_significant = df_category_chi2.category.tolist()[:10]\n",
    "data_train_vec = df_liwc_train[cols_significant]\n",
    "label_train = df_liwc_train['label']\n",
    "\n",
    "# data_test_vec = df_liwc_test.drop(columns=['word_length','label'])\n",
    "data_test_vec = df_liwc_test[cols_significant]\n",
    "label_test = df_liwc_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_disorder = LogisticRegression().fit(data_train_vec, label_train)\n",
    "model_disorder = XGBClassifier().fit(data_train_vec, label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(class_weight='balanced').fit(data_train_vec, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred_lr = model_lr.predict(data_test_vec)\n",
    "prob_pred_lr = model_lr.predict_proba(data_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(classification_report(label_test, label_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\" acc : {accuracy_score(label_test, label_pred_lr):.4f}\")\n",
    "logger.info(f\"  f1 : {f1_score(label_test, label_pred_lr):.4f}\")\n",
    "logger.info(f\"auroc: {roc_auc_score(label_test, prob_pred_lr[:,1]):.4f}\")\n",
    "logger.info(f\"auprc: {average_precision_score(label_test, prob_pred_lr[:,1]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(label_test, label_pred_lr, display_labels = ['Normal','Mental'], cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_word_importance = list(zip(df_liwc_train.columns, model_lr.coef_[0]))\n",
    "sorted(list_word_importance, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee9d3b8515634c3a0c24e5a00ed0069112c9f27bcf8e0589cc4d9d2654ef861a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
